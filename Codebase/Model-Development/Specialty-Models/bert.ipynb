{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47bc9cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found, using: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU found, using: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU not found\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbf19ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All outputs will be saved to: ./Saved-Models/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/training_run_2025-11-08_18-29-45\n"
     ]
    }
   ],
   "source": [
    "# MODEL_NAME = 'distilbert-base-uncased'\n",
    "# MODEL_NAME = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "# Small guy\n",
    "MODEL_NAME = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
    "# Big fella\n",
    "# Crashed GPU :(\n",
    "# MODEL_NAME = 'microsoft/BiomedNLP-BiomedBERT-large-uncased-abstract'\n",
    "CSV = '../../Data/Specialty-Data/specialty_data.csv'\n",
    "MAPPINGS = '../../Data/Specialty-Data/specialty_data_label_mappings.json'\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "ROOT_OUTPUT_DIR = f\"./Saved-Models/{MODEL_NAME}/training_run_{current_time}\"\n",
    "\n",
    "TRAINING_OUTPUT_DIRECTORY = os.path.join(ROOT_OUTPUT_DIR, 'results')\n",
    "MODEL_FINAL_DIRECTORY = os.path.join(ROOT_OUTPUT_DIR, 'final_model')\n",
    "LOGGING_DIRECTORY = os.path.join(ROOT_OUTPUT_DIR, 'logs')\n",
    "\n",
    "os.makedirs(TRAINING_OUTPUT_DIRECTORY, exist_ok=True)\n",
    "os.makedirs(MODEL_FINAL_DIRECTORY, exist_ok=True)\n",
    "os.makedirs(LOGGING_DIRECTORY, exist_ok=True)\n",
    "\n",
    "print(f\"All outputs will be saved to: {ROOT_OUTPUT_DIR}\")\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(\n",
    "        examples['transcription'], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "126ba691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-D M-MODE: , ,1.  Left atrial enlargement wit...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.  The left ventricular cavity size and wall ...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2-D ECHOCARDIOGRAM,Multiple views of the heart...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DESCRIPTION:,1.  Normal cardiac chambers size....</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2-D STUDY,1. Mild aortic stenosis, widely calc...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>EXAM: , Left heart cath, selective coronary an...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>INDICATION:,  Acute coronary syndrome.,CONSENT...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>ANGINA, is chest pain due to a lack of oxygen ...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>INDICATION: , Chest pain.,TYPE OF TEST: , Aden...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1263 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcription  \\\n",
       "0     2-D M-MODE: , ,1.  Left atrial enlargement wit...   \n",
       "1     1.  The left ventricular cavity size and wall ...   \n",
       "2     2-D ECHOCARDIOGRAM,Multiple views of the heart...   \n",
       "3     DESCRIPTION:,1.  Normal cardiac chambers size....   \n",
       "4     2-D STUDY,1. Mild aortic stenosis, widely calc...   \n",
       "...                                                 ...   \n",
       "1258  EXAM: , Left heart cath, selective coronary an...   \n",
       "1259  INDICATION:,  Acute coronary syndrome.,CONSENT...   \n",
       "1260  ANGINA, is chest pain due to a lack of oxygen ...   \n",
       "1261  INDICATION: , Chest pain.,TYPE OF TEST: , Aden...   \n",
       "1262  HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...   \n",
       "\n",
       "                medical_specialty  label  \n",
       "0      Cardiovascular / Pulmonary      0  \n",
       "1      Cardiovascular / Pulmonary      0  \n",
       "2      Cardiovascular / Pulmonary      0  \n",
       "3      Cardiovascular / Pulmonary      0  \n",
       "4      Cardiovascular / Pulmonary      0  \n",
       "...                           ...    ...  \n",
       "1258   Cardiovascular / Pulmonary      0  \n",
       "1259   Cardiovascular / Pulmonary      0  \n",
       "1260   Cardiovascular / Pulmonary      0  \n",
       "1261   Cardiovascular / Pulmonary      0  \n",
       "1262   Cardiovascular / Pulmonary      0  \n",
       "\n",
       "[1263 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    raw_df = pd.read_csv(CSV)\n",
    "    with open(MAPPINGS, 'r') as f:\n",
    "        specialty_and_id_map = json.load(f)\n",
    "except:\n",
    "    print(f\"Data not found, make sure to run the specialty_data_preprocessing.ipynb file in its entirety to retrieve the data\")\n",
    "# Retrieve labels\n",
    "label_to_id = specialty_and_id_map['label_to_id']\n",
    "id_to_label = {int(k): v for k, v in specialty_and_id_map['id_to_label'].items()}\n",
    "\n",
    "total_specialties = len(label_to_id)\n",
    "\n",
    "# Format dataframe for model\n",
    "df = raw_df[['transcription', 'medical_specialty', 'label']].dropna()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bdfd245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['transcription', 'medical_specialty', 'label'],\n",
       "        num_rows: 1010\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['transcription', 'medical_specialty', 'label'],\n",
       "        num_rows: 126\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['transcription', 'medical_specialty', 'label'],\n",
       "        num_rows: 127\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80% Train, 10% Validation, 10% Test\n",
    "train_df, test_val_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    stratify=df['label'], \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    test_val_df, \n",
    "    test_size=0.5, \n",
    "    stratify=test_val_df['label'], \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "    'validation': Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
    "    'test': Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "})\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24d8ab69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aaf4a1ad3934d50989d23d051de1adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1010 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721ff05f2d224e6ab3f6902c7a7f68ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53df1524957040a2896ca5bd609ffcea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1010\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 126\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 127\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenized_ds = ds.map(lambda examples: tokenize_function(examples, tokenizer), batched=True)\n",
    "\n",
    "tokenized_ds = tokenized_ds.remove_columns(['transcription', 'medical_specialty'])\n",
    "\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7965180",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "The paging file is too small for this operation to complete. (os error 1455)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Include this in the model for Bio_ClinicalBERT to force the use of safetensors vs using insecure load\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# May not be necessary on other models\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# use_safetensors=True,\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_specialties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid_to_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel2id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_to_id\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m training_args = TrainingArguments(\n\u001b[32m     14\u001b[39m     output_dir=TRAINING_OUTPUT_DIRECTORY,\n\u001b[32m     15\u001b[39m     num_train_epochs=\u001b[32m7\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     report_to=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m trainer = Trainer(\n\u001b[32m     32\u001b[39m     model=model,\n\u001b[32m     33\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     tokenizer=tokenizer,\n\u001b[32m     38\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\transformers\\modeling_utils.py:4925\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4922\u001b[39m is_from_file = pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4924\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_from_file \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sharded \u001b[38;5;129;01mand\u001b[39;00m checkpoint_files[\u001b[32m0\u001b[39m].endswith(\u001b[33m\"\u001b[39m\u001b[33m.safetensors\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4925\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   4926\u001b[39m         metadata = f.metadata()\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4929\u001b[39m         \u001b[38;5;66;03m# Assume it's a pytorch checkpoint (introduced for timm checkpoints)\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: The paging file is too small for this operation to complete. (os error 1455)"
     ]
    }
   ],
   "source": [
    "# Include this in the model for Bio_ClinicalBERT to force the use of safetensors vs using insecure load\n",
    "# May not be necessary on other models\n",
    "# use_safetensors=True,\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    use_safetensors=True,\n",
    "    num_labels=total_specialties,\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=TRAINING_OUTPUT_DIRECTORY,\n",
    "    num_train_epochs=7,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=LOGGING_DIRECTORY,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213602c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2194, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1179, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1050, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 858, in structured_traceback\n",
      "    formatted_exceptions: list[list[str]] = self.format_exception_as_a_whole(\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 743, in format_exception_as_a_whole\n",
      "    records = self.get_records(etb, context, tb_offset) if etb else []\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 845, in get_records\n",
      "    res = list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\stack_data\\core.py\", line 597, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"c:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\stack_data\\utils.py\", line 83, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"c:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\stack_data\\core.py\", line 587, in mapper\n",
      "    return cls(f, options)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\stack_data\\core.py\", line 551, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\executing\\executing.py\", line 224, in executing\n",
      "    source = cls.for_frame(frame)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\executing\\executing.py\", line 143, in for_frame\n",
      "    return cls.for_filename(frame.f_code.co_filename, frame.f_globals or {}, use_cache)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\executing\\executing.py\", line 172, in for_filename\n",
      "    return cls._for_filename_and_lines(filename, tuple(lines))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\executing\\executing.py\", line 183, in _for_filename_and_lines\n",
      "    result = source_cache[(filename, lines)] = cls(filename, lines)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\site-packages\\executing\\executing.py\", line 123, in __init__\n",
      "    self.tree = ast.parse(self.text, filename=filename)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\spenc\\anaconda3\\envs\\interview_env\\Lib\\ast.py\", line 52, in parse\n",
      "    return compile(source, filename, mode, flags,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Model\")\n",
    "trainer.train()\n",
    "print(f\"Training Complete\")\n",
    "\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "print(f\"Evaluating on validation dataset\")\n",
    "test_results = trainer.evaluate(tokenized_ds[\"test\"])\n",
    "\n",
    "print(f\"Validation results\")\n",
    "print(test_results)\n",
    "\n",
    "with open(f\"{TRAINING_OUTPUT_DIRECTORY}/test_results.json\", 'w') as f:\n",
    "    json.dump(test_results, f, indent=4)\n",
    "\n",
    "print(f\"Saving final model to {MODEL_FINAL_DIRECTORY}\")\n",
    "trainer.save_model(MODEL_FINAL_DIRECTORY)\n",
    "print(f\"Model saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
