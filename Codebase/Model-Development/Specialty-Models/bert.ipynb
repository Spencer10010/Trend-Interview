{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47bc9cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found, using: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU found, using: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU not found\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbf19ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All outputs will be saved to: ./Saved-Models/BERT/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/training_run_2025-11-10_13-17-40\n"
     ]
    }
   ],
   "source": [
    "# Take a look at a few different pretrained hugging face models and set up output directories\n",
    "# MODEL_NAME = 'distilbert-base-uncased'\n",
    "# MODEL_NAME = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "# Small guy\n",
    "MODEL_NAME = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
    "# Big fella\n",
    "# Crashed GPU :(\n",
    "# MODEL_NAME = 'microsoft/BiomedNLP-BiomedBERT-large-uncased-abstract'\n",
    "CSV = '../../Data/Specialty-Data/specialty_data.csv'\n",
    "MAPPINGS = '../../Data/Specialty-Data/specialty_data_label_mappings.json'\n",
    "WEIGHTS = '../../Data/Specialty-Data/specialty_data_class_weights.json'\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "ROOT_OUTPUT_DIR = f\"./Saved-Models/BERT/{MODEL_NAME}/training_run_{current_time}\"\n",
    "\n",
    "TRAINING_OUTPUT_DIRECTORY = os.path.join(ROOT_OUTPUT_DIR, 'results')\n",
    "MODEL_FINAL_DIRECTORY = os.path.join(ROOT_OUTPUT_DIR, 'final_model')\n",
    "LOGGING_DIRECTORY = os.path.join(ROOT_OUTPUT_DIR, 'logs')\n",
    "\n",
    "os.makedirs(TRAINING_OUTPUT_DIRECTORY, exist_ok=True)\n",
    "os.makedirs(MODEL_FINAL_DIRECTORY, exist_ok=True)\n",
    "os.makedirs(LOGGING_DIRECTORY, exist_ok=True)\n",
    "\n",
    "print(f\"All outputs will be saved to: {ROOT_OUTPUT_DIR}\")\n",
    "\n",
    "# These functions will be used for the BERT models\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(\n",
    "        examples['transcription'], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    acc = accuracy_score(labels, predictions)    \n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    \n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "126ba691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcription</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2-D M-MODE: , ,1.  Left atrial enlargement wit...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.  The left ventricular cavity size and wall ...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2-D ECHOCARDIOGRAM,Multiple views of the heart...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DESCRIPTION:,1.  Normal cardiac chambers size....</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2-D STUDY,1. Mild aortic stenosis, widely calc...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>EXAM: , Left heart cath, selective coronary an...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>INDICATION:,  Acute coronary syndrome.,CONSENT...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>ANGINA, is chest pain due to a lack of oxygen ...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>INDICATION: , Chest pain.,TYPE OF TEST: , Aden...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1263 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcription  \\\n",
       "0     2-D M-MODE: , ,1.  Left atrial enlargement wit...   \n",
       "1     1.  The left ventricular cavity size and wall ...   \n",
       "2     2-D ECHOCARDIOGRAM,Multiple views of the heart...   \n",
       "3     DESCRIPTION:,1.  Normal cardiac chambers size....   \n",
       "4     2-D STUDY,1. Mild aortic stenosis, widely calc...   \n",
       "...                                                 ...   \n",
       "1258  EXAM: , Left heart cath, selective coronary an...   \n",
       "1259  INDICATION:,  Acute coronary syndrome.,CONSENT...   \n",
       "1260  ANGINA, is chest pain due to a lack of oxygen ...   \n",
       "1261  INDICATION: , Chest pain.,TYPE OF TEST: , Aden...   \n",
       "1262  HISTORY OF PRESENT ILLNESS: , Mr. ABC is a 60-...   \n",
       "\n",
       "                medical_specialty  label  \n",
       "0      Cardiovascular / Pulmonary      0  \n",
       "1      Cardiovascular / Pulmonary      0  \n",
       "2      Cardiovascular / Pulmonary      0  \n",
       "3      Cardiovascular / Pulmonary      0  \n",
       "4      Cardiovascular / Pulmonary      0  \n",
       "...                           ...    ...  \n",
       "1258   Cardiovascular / Pulmonary      0  \n",
       "1259   Cardiovascular / Pulmonary      0  \n",
       "1260   Cardiovascular / Pulmonary      0  \n",
       "1261   Cardiovascular / Pulmonary      0  \n",
       "1262   Cardiovascular / Pulmonary      0  \n",
       "\n",
       "[1263 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the data\n",
    "try:\n",
    "    raw_df = pd.read_csv(CSV)\n",
    "    with open(MAPPINGS, 'r') as f:\n",
    "        specialty_and_id_map = json.load(f)\n",
    "    with open(WEIGHTS, 'r') as f:\n",
    "        class_weights = json.load(f)\n",
    "except:\n",
    "    print(f\"Data not found, make sure to run the specialty_data_preprocessing.ipynb file in its entirety to retrieve the data\")\n",
    "\n",
    "# Retrieve labels\n",
    "label_to_id = specialty_and_id_map['label_to_id']\n",
    "id_to_label = {int(k): v for k, v in specialty_and_id_map['id_to_label'].items()}\n",
    "\n",
    "total_specialties = len(label_to_id)\n",
    "\n",
    "# Format dataframe for model\n",
    "df = raw_df[['transcription', 'medical_specialty', 'label']].dropna()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bdfd245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['transcription', 'medical_specialty', 'label'],\n",
       "        num_rows: 1010\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['transcription', 'medical_specialty', 'label'],\n",
       "        num_rows: 126\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['transcription', 'medical_specialty', 'label'],\n",
       "        num_rows: 127\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80% Train, 10% Validation, 10% Test\n",
    "train_df, test_val_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    stratify=df['label'], \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    test_val_df, \n",
    "    test_size=0.5, \n",
    "    stratify=test_val_df['label'], \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "    'validation': Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
    "    'test': Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "})\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d8ab69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67468bd4eb264f41877bcd053d508c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1010 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a8d4db76f84b9da84ab8be450b94fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/126 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02fba9c9c8c343b58bad0bbb662d7561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/127 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1010\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 126\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 127\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tokenizer based on transcriptions\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenized_ds = ds.map(lambda examples: tokenize_function(examples, tokenizer), batched=True)\n",
    "\n",
    "tokenized_ds = tokenized_ds.remove_columns(['transcription', 'medical_specialty'])\n",
    "\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7965180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\spenc\\AppData\\Local\\Temp\\ipykernel_26748\\1055306954.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Include the below input in the model for Bio_ClinicalBERT to force the use of safetensors vs using insecure load\n",
    "# May not be necessary on other models\n",
    "# use_safetensors=True,\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    use_safetensors=True,\n",
    "    num_labels=total_specialties,\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "# 1e-5 too slow for meaningful improvements and 3e-5 is too fast\n",
    "# Epochs appear to level out after approximately 10, reached peak of 30 epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=TRAINING_OUTPUT_DIRECTORY,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=LOGGING_DIRECTORY,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Can use either default Trainer or create Trainer that punishes the model based on the weights of its mistakes\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Either use default Trainer or WeightedTrainer, same inputs\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "213602c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='512' max='512' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [512/512 08:56, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.640900</td>\n",
       "      <td>1.546859</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.393233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.502800</td>\n",
       "      <td>1.217228</td>\n",
       "      <td>0.603175</td>\n",
       "      <td>0.575155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.237500</td>\n",
       "      <td>0.649896</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.784985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.595400</td>\n",
       "      <td>0.499204</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.787128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.487900</td>\n",
       "      <td>0.494533</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.795137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.513888</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.749323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.412600</td>\n",
       "      <td>0.583631</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.720361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.331900</td>\n",
       "      <td>0.551572</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.774678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete\n",
      "-------------------------------------\n",
      "Evaluating on validation dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results\n",
      "{'eval_loss': 0.7071400880813599, 'eval_accuracy': 0.7559055118110236, 'eval_f1': 0.7523039802951461, 'eval_runtime': 4.0175, 'eval_samples_per_second': 31.612, 'eval_steps_per_second': 1.991, 'epoch': 8.0}\n",
      "Saving final model to ./Saved-Models/BERT/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext/training_run_2025-11-10_13-17-40\\final_model\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# This is where the training is conducted, can retrieve the final model from the printed directory\n",
    "# They're pretty heavy on storage coming out to 5 gigs a piece\n",
    "print(f\"Training Model\")\n",
    "trainer.train()\n",
    "print(f\"Training Complete\")\n",
    "\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "print(f\"Evaluating on validation dataset\")\n",
    "test_results = trainer.evaluate(tokenized_ds[\"test\"])\n",
    "\n",
    "print(f\"Validation results\")\n",
    "print(test_results)\n",
    "\n",
    "with open(f\"{TRAINING_OUTPUT_DIRECTORY}/test_results.json\", 'w') as f:\n",
    "    json.dump(test_results, f, indent=4)\n",
    "\n",
    "print(f\"Saving final model to {MODEL_FINAL_DIRECTORY}\")\n",
    "trainer.save_model(MODEL_FINAL_DIRECTORY)\n",
    "print(f\"Model saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
